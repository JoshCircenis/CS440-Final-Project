{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 08:30:35.124718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733326235.138362 1385758 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733326235.142634 1385758 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 08:30:35.158142: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tf_keras\n",
    "from keras import layers\n",
    "from transformers import BertTokenizerFast, BertConfig, TFBertModel, RobertaTokenizerFast, RobertaConfig, TFRobertaModel\n",
    "\n",
    "gpu = 0\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'java_train': 'data/java_train-00000-of-00001.parquet', 'java_test': 'data/java_test-00000-of-00001.parquet', 'python_train': 'data/python_train-00000-of-00001.parquet', 'python_test': 'data/python_test-00000-of-00001.parquet', 'pharo_train': 'data/pharo_train-00000-of-00001.parquet', 'pharo_test': 'data/pharo_test-00000-of-00001.parquet'}\n",
    "\n",
    "java_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_train\"])\n",
    "python_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_train\"])\n",
    "pharo_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_train\"])\n",
    "\n",
    "java_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_test\"])\n",
    "python_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_test\"])\n",
    "pharo_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733326250.473420 1385758 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 77661 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'bert-base-uncased'\n",
    "\n",
    "# config = BertConfig.from_pretrained(model_name)\n",
    "# config.output_hidden_states = False\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name, config=config)\n",
    "# transformer_model = TFBertModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "model_name = 'microsoft/codebert-base'\n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, config=config)\n",
    "transformer_model = TFRobertaModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   1246456   ['input_ids[0][0]']           \n",
      " r)                          ngAndCrossAttentions(last_   32                                      \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)     (None, 768)                  0         ['roberta[0][1]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   49152     ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 64)                   49152     ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   49152     ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 64)                   192       ['dense[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 64)                   192       ['dense_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 64)                   192       ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 64)                   0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 64)                   0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 64)                   0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " java (Dense)                (None, 7)                    455       ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " pharo (Dense)               (None, 7)                    455       ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " python (Dense)              (None, 5)                    325       ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124794899 (476.05 MB)\n",
      "Trainable params: 124794515 (476.05 MB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert = transformer_model.layers[0]\n",
    "\n",
    "input_ids = tf_keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "# attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \n",
    "# inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Load the bert model as a layer\n",
    "bert_model = bert(inputs)[1]\n",
    "dropout = tf_keras.layers.Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "# Additional dense layers\n",
    "javaDense = tf_keras.layers.Dense(64, kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), use_bias=False)(pooled_output)\n",
    "javaDense = tf_keras.layers.BatchNormalization(center=True, scale=False)(javaDense)\n",
    "javaDense = tf_keras.layers.Activation('relu')(javaDense)\n",
    "\n",
    "pythonDense = tf_keras.layers.Dense(64, kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), use_bias=False)(pooled_output)\n",
    "pythonDense = tf_keras.layers.BatchNormalization(center=True, scale=False)(pythonDense)\n",
    "pythonDense = tf_keras.layers.Activation('relu')(pythonDense)\n",
    "\n",
    "pharoDense = tf_keras.layers.Dense(64, kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), use_bias=False)(pooled_output)\n",
    "pharoDense = tf_keras.layers.BatchNormalization(center=True, scale=False)(pharoDense)\n",
    "pharoDense = tf_keras.layers.Activation('relu')(pharoDense)\n",
    "\n",
    "# Build model output\n",
    "java = tf_keras.layers.Dense(units=7, kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), name='java')(javaDense)\n",
    "python = tf_keras.layers.Dense(units=5, kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), name='python')(pythonDense)\n",
    "pharo = tf_keras.layers.Dense(units=7, kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), name='pharo')(pharoDense)\n",
    "outputs = {'java': java, 'python': python, 'pharo': pharo}\n",
    "\n",
    "# And combine it all in a model object\n",
    "model = tf_keras.models.Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
    "\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer):\n",
    "    inputs = tokenizer(df, padding='max_length', truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "java_train_inputs = tokenize_data(java_train.combo.tolist(), tokenizer)\n",
    "python_train_inputs = tokenize_data(python_train.combo.tolist(), tokenizer)\n",
    "pharo_train_inputs = tokenize_data(pharo_train.combo.tolist(), tokenizer)\n",
    "\n",
    "java_test_inputs = tokenize_data(java_test.combo.tolist(), tokenizer)\n",
    "python_test_inputs = tokenize_data(python_test.combo.tolist(), tokenizer)\n",
    "pharo_test_inputs = tokenize_data(pharo_test.combo.tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "javaLen = len(java_train.labels.tolist())\n",
    "pythonLen = len(python_train.labels.tolist())\n",
    "pharoLen = len(pharo_train.labels.tolist())\n",
    "\n",
    "java_train_labels = {'java': java_train.labels.tolist(), 'python': [[np.int64(0)]*5]*javaLen, 'pharo': [[np.int64(0)]*7]*javaLen}\n",
    "python_train_labels = {'java': [[np.int64(0)]*7]*pythonLen, 'python': python_train.labels.tolist(), 'pharo': [[np.int64(0)]*7]*pythonLen}\n",
    "pharo_train_labels = {'java': [[np.int64(0)]*7]*pharoLen, 'python': [[np.int64(0)]*5]*pharoLen, 'pharo': pharo_train.labels.tolist()}\n",
    "\n",
    "javaLen = len(java_test.labels.tolist())\n",
    "pythonLen = len(python_test.labels.tolist())\n",
    "pharoLen = len(pharo_test.labels.tolist())\n",
    "\n",
    "java_test_labels = {'java': java_test.labels.tolist(), 'python': [[np.int64(0)]*5]*javaLen, 'pharo': [[np.int64(0)]*7]*javaLen}\n",
    "python_test_labels = {'java': [[np.int64(0)]*7]*pythonLen, 'python': python_test.labels.tolist(), 'pharo': [[np.int64(0)]*7]*pythonLen}\n",
    "pharo_test_labels = {'java': [[np.int64(0)]*7]*pharoLen, 'python': [[np.int64(0)]*5]*pharoLen, 'pharo': pharo_test.labels.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(inputs, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((dict(inputs), dict(labels)))\n",
    "\n",
    "java_train_dataset = create_tf_dataset(java_train_inputs, java_train_labels)\n",
    "python_train_dataset = create_tf_dataset(python_train_inputs, python_train_labels)\n",
    "pharo_train_dataset = create_tf_dataset(pharo_train_inputs, pharo_train_labels)\n",
    "\n",
    "java_test_dataset = create_tf_dataset(java_test_inputs, java_test_labels)\n",
    "python_test_dataset = create_tf_dataset(python_test_inputs, python_test_labels)\n",
    "pharo_test_dataset = create_tf_dataset(pharo_test_inputs, pharo_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectiveLoss(y_true, y_pred):\n",
    "    if keras.backend.all(keras.backend.equal(y_true, 0)):\n",
    "        return 0.0\n",
    "    else:\n",
    "        loss = tf_keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "        return loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/top/students/UNGRAD/Other/jecircen/home/.conda/envs/tf440/lib/python3.10/site-packages/tf_keras/src/engine/functional.py:641: UserWarning: Input dict contained keys ['attention_mask'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733326606.258864 1388464 service.cc:148] XLA service 0x7f34c1b5a6b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733326606.258896 1388464 service.cc:156]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-12-04 08:36:46.264109: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1733326606.282503 1388464 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1733326606.342543 1388464 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 92s 49ms/step - loss: 1.7805 - java_loss: 1.2291 - pharo_loss: 0.2513 - python_loss: 0.3000 - java_accuracy: 0.6087 - pharo_accuracy: 0.1189 - python_accuracy: 0.3025\n",
      "Epoch 2/5\n",
      "1350/1350 [==============================] - 66s 49ms/step - loss: 1.6071 - java_loss: 1.0703 - pharo_loss: 0.2392 - python_loss: 0.2976 - java_accuracy: 0.6291 - pharo_accuracy: 0.0502 - python_accuracy: 0.4610\n",
      "Epoch 3/5\n",
      "1350/1350 [==============================] - 66s 49ms/step - loss: 1.5740 - java_loss: 1.0425 - pharo_loss: 0.2348 - python_loss: 0.2967 - java_accuracy: 0.6290 - pharo_accuracy: 0.0502 - python_accuracy: 0.4599\n",
      "Epoch 4/5\n",
      "1350/1350 [==============================] - 66s 49ms/step - loss: 1.5592 - java_loss: 1.0291 - pharo_loss: 0.2327 - python_loss: 0.2973 - java_accuracy: 0.6292 - pharo_accuracy: 0.0502 - python_accuracy: 0.4877\n",
      "Epoch 5/5\n",
      "1350/1350 [==============================] - 66s 49ms/step - loss: 1.5540 - java_loss: 1.0257 - pharo_loss: 0.2313 - python_loss: 0.2971 - java_accuracy: 0.6288 - pharo_accuracy: 0.0502 - python_accuracy: 0.4179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f35ea71c4f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-5, epsilon=1e-8)\n",
    "#loss = {'java': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'python': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'pharo': tf_keras.losses.CategoricalCrossentropy(from_logits = True)}\n",
    "loss = {'java': selectiveLoss, 'python': selectiveLoss, 'pharo': selectiveLoss}\n",
    "metric = {'java': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'python': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'pharo': tf_keras.metrics.CategoricalAccuracy('accuracy')}\n",
    "\n",
    "model.layers[1].trainable = False \n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "# Concatenate all datasets\n",
    "train_dataset = java_train_dataset.concatenate(python_train_dataset).concatenate(pharo_train_dataset)\n",
    "\n",
    "# Shuffle and batch the training dataset\n",
    "train_dataset = train_dataset.shuffle(1000).batch(8).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/top/students/UNGRAD/Other/jecircen/home/.conda/envs/tf440/lib/python3.10/site-packages/tf_keras/src/engine/functional.py:641: UserWarning: Input dict contained keys ['attention_mask'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1350 [..............................] - ETA: 11:40:41 - loss: 1.5208 - java_loss: 1.5208 - pharo_loss: 0.0000e+00 - python_loss: 0.0000e+00 - java_accuracy: 0.3750 - pharo_accuracy: 0.0000e+00 - python_accuracy: 0.5000"
     ]
    }
   ],
   "source": [
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-5, epsilon=1e-8)\n",
    "#loss = {'java': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'python': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'pharo': tf_keras.losses.CategoricalCrossentropy(from_logits = True)}\n",
    "loss = {'java': selectiveLoss, 'python': selectiveLoss, 'pharo': selectiveLoss}\n",
    "metric = {'java': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'python': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'pharo': tf_keras.metrics.CategoricalAccuracy('accuracy')}\n",
    "\n",
    "model.layers[1].trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "# Concatenate all datasets\n",
    "train_dataset = java_train_dataset.concatenate(python_train_dataset).concatenate(pharo_train_dataset)\n",
    "\n",
    "# Shuffle and batch the training dataset\n",
    "train_dataset = train_dataset.shuffle(1000).batch(8).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/top/students/UNGRAD/Other/jecircen/home/.conda/envs/tf440/lib/python3.10/site-packages/tf_keras/src/engine/functional.py:641: UserWarning: Input dict contained keys ['token_type_ids', 'attention_mask'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['pharo/kernel:0', 'pharo/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476/476 [==============================] - 48s 84ms/step - loss: 1.4861 - java_loss: 1.4861 - java_accuracy: 0.4758\n",
      "Epoch 2/5\n",
      "476/476 [==============================] - 40s 84ms/step - loss: 1.4823 - java_loss: 1.4823 - java_accuracy: 0.4741\n",
      "Epoch 3/5\n",
      "476/476 [==============================] - 40s 85ms/step - loss: 1.4794 - java_loss: 1.4794 - java_accuracy: 0.4727\n",
      "Epoch 4/5\n",
      "476/476 [==============================] - 40s 85ms/step - loss: 1.4858 - java_loss: 1.4858 - java_accuracy: 0.4699\n",
      "Epoch 5/5\n",
      "476/476 [==============================] - 40s 85ms/step - loss: 1.4926 - java_loss: 1.4926 - java_accuracy: 0.4685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7fdf9e8d2650>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-4, epsilon=1e-8, weight_decay=0.01, clipnorm=1.0)\n",
    "#loss = {'java': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'python': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'pharo': tf_keras.losses.CategoricalCrossentropy(from_logits = True)}\n",
    "loss = {'java': selectiveLoss, 'python': None, 'pharo': None}\n",
    "metric = {'java': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'python': None, 'pharo': None}\n",
    "\n",
    "model.layers[1].trainable = False \n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "# Shuffle and batch the training dataset\n",
    "train_dataset = java_train_dataset.shuffle(1000).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/top/students/UNGRAD/Other/jecircen/home/.conda/envs/tf440/lib/python3.10/site-packages/tf_keras/src/engine/functional.py:641: UserWarning: Input dict contained keys ['token_type_ids', 'attention_mask'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'pharo/kernel:0', 'pharo/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 17s 84ms/step - loss: 1.7332 - python_loss: 1.7332 - python_accuracy: 0.2840\n",
      "Epoch 2/5\n",
      "118/118 [==============================] - 10s 84ms/step - loss: 1.7389 - python_loss: 1.7389 - python_accuracy: 0.3052\n",
      "Epoch 3/5\n",
      "118/118 [==============================] - 10s 84ms/step - loss: 1.7420 - python_loss: 1.7420 - python_accuracy: 0.2797\n",
      "Epoch 4/5\n",
      "118/118 [==============================] - 10s 84ms/step - loss: 1.7245 - python_loss: 1.7245 - python_accuracy: 0.2988\n",
      "Epoch 5/5\n",
      "118/118 [==============================] - 10s 84ms/step - loss: 1.7469 - python_loss: 1.7469 - python_accuracy: 0.2935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7fdf83c3b160>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-4, epsilon=1e-8, weight_decay=0.01, clipnorm=1.0)\n",
    "#loss = {'java': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'python': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'pharo': tf_keras.losses.CategoricalCrossentropy(from_logits = True)}\n",
    "loss = {'java': None, 'python': selectiveLoss, 'pharo': None}\n",
    "metric = {'java': None, 'python': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'pharo': None}\n",
    "\n",
    "model.layers[1].trainable = False \n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "# Shuffle and batch the training dataset\n",
    "train_dataset = python_train_dataset.shuffle(1000).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['java/kernel:0', 'java/bias:0', 'python/kernel:0', 'python/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 14s 83ms/step - loss: 1.9962 - pharo_loss: 1.9962 - pharo_accuracy: 0.3852\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 7s 84ms/step - loss: 1.9629 - pharo_loss: 1.9629 - pharo_accuracy: 0.3991\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 7s 84ms/step - loss: 1.9735 - pharo_loss: 1.9735 - pharo_accuracy: 0.4068\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 7s 84ms/step - loss: 1.9839 - pharo_loss: 1.9839 - pharo_accuracy: 0.4176\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 7s 84ms/step - loss: 1.9713 - pharo_loss: 1.9713 - pharo_accuracy: 0.4106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7fdf9f4d5630>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-4, epsilon=1e-8, weight_decay=0.01, clipnorm=1.0)\n",
    "#loss = {'java': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'python': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'pharo': tf_keras.losses.CategoricalCrossentropy(from_logits = True)}\n",
    "loss = {'java': None, 'python': None, 'pharo': selectiveLoss}\n",
    "metric = {'java': None, 'python': None, 'pharo': tf_keras.metrics.CategoricalAccuracy('accuracy')}\n",
    "\n",
    "model.layers[1].trainable = False \n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "# Shuffle and batch the training dataset\n",
    "train_dataset = pharo_train_dataset.shuffle(1000).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-4, epsilon=1e-8, weight_decay=0.01, clipnorm=1.0)\n",
    "#loss = {'java': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'python': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'pharo': tf_keras.losses.CategoricalCrossentropy(from_logits = True)}\n",
    "loss = {'java': selectiveLoss, 'python': selectiveLoss, 'pharo': selectiveLoss}\n",
    "metric = {'java': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'python': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'pharo': tf_keras.metrics.CategoricalAccuracy('accuracy')}\n",
    "\n",
    "model.layers[1].trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 11s 40ms/step - loss: 1.4261 - java_loss: 1.4261 - pharo_loss: 0.0000e+00 - python_loss: 0.0000e+00 - java_accuracy: 0.5194 - pharo_accuracy: 0.0000e+00 - python_accuracy: 0.0435\n",
      "51/51 [==============================] - 2s 40ms/step - loss: 1.6987 - java_loss: 0.0000e+00 - pharo_loss: 0.0000e+00 - python_loss: 1.6987 - java_accuracy: 1.0000 - pharo_accuracy: 0.0000e+00 - python_accuracy: 0.3202\n",
      "37/37 [==============================] - 1s 40ms/step - loss: 1.6986 - java_loss: 0.0000e+00 - pharo_loss: 1.6986 - python_loss: 0.0000e+00 - java_accuracy: 1.0000 - pharo_accuracy: 0.4118 - python_accuracy: 0.0138\n",
      "Java Test Accuracy: 0.5194202661514282\n",
      "Python Test Accuracy: 0.32019704580307007\n",
      "Pharo Test Accuracy: 0.4117647111415863\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test datasets\n",
    "# test_dataset = java_test_dataset.concatenate(python_test_dataset).concatenate(pharo_test_dataset)\n",
    "# test_acc = model.evaluate(test_dataset.batch(16))\n",
    "java_results = model.evaluate(java_test_dataset.batch(8))\n",
    "python_results = model.evaluate(python_test_dataset.batch(8))\n",
    "pharo_results = model.evaluate(pharo_test_dataset.batch(8))\n",
    "\n",
    "print(f\"Java Test Accuracy: {java_results[-3]}\")\n",
    "print(f\"Python Test Accuracy: {python_results[-1]}\")\n",
    "print(f\"Pharo Test Accuracy: {pharo_results[-2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  tf_keras.optimizers.AdamW([\n",
    "    {'params': model.layers[1].parameters(), 'lr': 3e-5},  # For BERT layers\n",
    "    {'params': model.layers[3].parameters(), 'lr': 5e-4},  # For classification heads java\n",
    "    {'params': model.layers[4].parameters(), 'lr': 5e-4},  # For classification heads pharo\n",
    "    {'params': model.layers[5].parameters(), 'lr': 5e-4}  # For classification heads python\n",
    "], eps=1e-8, weight_decay=0.01, clipnorm=1.0)\n",
    "#loss = {'java': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'python': tf_keras.losses.CategoricalCrossentropy(from_logits = True), 'pharo': tf_keras.losses.CategoricalCrossentropy(from_logits = True)}\n",
    "loss = {'java': selectiveLoss, 'python': selectiveLoss, 'pharo': selectiveLoss}\n",
    "metric = {'java': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'python': tf_keras.metrics.CategoricalAccuracy('accuracy'), 'pharo': tf_keras.metrics.CategoricalAccuracy('accuracy')}\n",
    "\n",
    "model.layers[1].trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "# Concatenate all datasets\n",
    "train_dataset = java_train_dataset.concatenate(python_train_dataset).concatenate(pharo_train_dataset)\n",
    "\n",
    "# Shuffle and batch the training dataset\n",
    "train_dataset = train_dataset.shuffle(1000).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test datasets\n",
    "test_dataset = java_test_dataset.concatenate(python_test_dataset).concatenate(pharo_test_dataset)\n",
    "test_acc = model.evaluate(test_dataset.batch(16))\n",
    "\n",
    "print(f\"Java Test Accuracy: {test_acc[-3]}\")\n",
    "print(f\"Python Test Accuracy: {test_acc[-2]}\")\n",
    "print(f\"Pharo Test Accuracy: {test_acc[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf440",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
