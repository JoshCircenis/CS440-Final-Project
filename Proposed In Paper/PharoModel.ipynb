{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:53:50.823086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733360030.836903 1622778 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733360030.841121 1622778 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:53:50.856806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tf_keras\n",
    "from keras import layers\n",
    "from transformers import RobertaTokenizerFast, RobertaConfig, TFRobertaModel\n",
    "\n",
    "gpu = 1\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733360038.303328 1622778 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78777 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:c1:00.0, compute capability: 8.0\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/codebert-base'\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, config=config)\n",
    "transformer_model = TFRobertaModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   1246456   ['attention_mask[0][0]',      \n",
      " r)                          ngAndCrossAttentions(last_   32         'input_ids[0][0]']           \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)     (None, 768)                  0         ['roberta[0][1]']             \n",
      "                                                                                                  \n",
      " pharo (Dense)               (None, 7)                    5383      ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124651015 (475.51 MB)\n",
      "Trainable params: 124651015 (475.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert = transformer_model.layers[0]\n",
    "\n",
    "# Create input layer for tokenized data\n",
    "input_ids = tf_keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "attention_mask = tf_keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32') \n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "# inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Load the bert model as a layer\n",
    "bert_model = bert(inputs)[1]\n",
    "dropout = tf_keras.layers.Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "outputs = tf_keras.layers.Dense(units=7, activation=\"softmax\", kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), name='pharo')(pooled_output)\n",
    "model = tf_keras.models.Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'java_train': 'data/java_train-00000-of-00001.parquet', 'java_test': 'data/java_test-00000-of-00001.parquet', 'python_train': 'data/python_train-00000-of-00001.parquet', 'python_test': 'data/python_test-00000-of-00001.parquet', 'pharo_train': 'data/pharo_train-00000-of-00001.parquet', 'pharo_test': 'data/pharo_test-00000-of-00001.parquet'}\n",
    "\n",
    "pharo_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_train\"])\n",
    "pharo_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer):\n",
    "    # Tokenize data and truncate/pad to 512\n",
    "    inputs = tokenizer(df, padding='max_length', truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Tokenize all data\n",
    "pharo_train_inputs = tokenize_data(pharo_train.combo.tolist(), tokenizer)\n",
    "pharo_test_inputs = tokenize_data(pharo_test.combo.tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels in list form\n",
    "pharo_train_labels = pharo_train.labels.tolist()\n",
    "pharo_test_labels = pharo_test.labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from the preprocessed data\n",
    "def create_tf_dataset(inputs, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((dict(inputs), labels))\n",
    "\n",
    "pharo_train_dataset = create_tf_dataset(pharo_train_inputs, pharo_train_labels)\n",
    "pharo_test_dataset = create_tf_dataset(pharo_test_inputs, pharo_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   1246456   ['attention_mask[0][0]',      \n",
      " r)                          ngAndCrossAttentions(last_   32         'input_ids[0][0]']           \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)     (None, 768)                  0         ['roberta[0][1]']             \n",
      "                                                                                                  \n",
      " pharo (Dense)               (None, 7)                    5383      ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124651015 (475.51 MB)\n",
      "Trainable params: 5383 (21.03 KB)\n",
      "Non-trainable params: 124645632 (475.49 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with RoBERTa base frozen\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-5, epsilon=1e-8)\n",
    "loss = tf_keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "metric = [tf_keras.metrics.CategoricalAccuracy('accuracy'), tf_keras.metrics.TruePositives(), tf_keras.metrics.TrueNegatives(), tf_keras.metrics.FalsePositives(), tf_keras.metrics.FalseNegatives()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/top/students/UNGRAD/Other/jecircen/home/.conda/envs/tf440/lib/python3.10/site-packages/tf_keras/src/backend.py:5577: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733360065.480496 1684088 service.cc:148] XLA service 0x7f24ad1051c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733360065.480522 1684088 service.cc:156]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-12-04 17:54:25.485751: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1733360065.500847 1684088 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1733360065.573224 1684088 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 24s 99ms/step - loss: 1.9892 - accuracy: 0.4129 - true_positives: 0.0000e+00 - true_negatives: 7629.0000 - false_positives: 0.0000e+00 - false_negatives: 1457.0000\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 17s 105ms/step - loss: 1.9109 - accuracy: 0.4168 - true_positives: 0.0000e+00 - true_negatives: 7629.0000 - false_positives: 0.0000e+00 - false_negatives: 1457.0000\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 17s 105ms/step - loss: 1.9193 - accuracy: 0.4183 - true_positives: 9.0000 - true_negatives: 7624.0000 - false_positives: 5.0000 - false_negatives: 1448.0000\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 17s 105ms/step - loss: 1.9149 - accuracy: 0.4176 - true_positives: 0.0000e+00 - true_negatives: 7627.0000 - false_positives: 2.0000 - false_negatives: 1457.0000\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 17s 105ms/step - loss: 1.9138 - accuracy: 0.4176 - true_positives: 5.0000 - true_negatives: 7626.0000 - false_positives: 3.0000 - false_negatives: 1452.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f25bf162230>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle and batch the training data use autotune for faster execution\n",
    "train_dataset = pharo_train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "163/163 [==============================] - 52s 157ms/step - loss: 1.5183 - accuracy: 0.5609 - true_positives_1: 442.0000 - true_negatives_1: 7502.0000 - false_positives_1: 127.0000 - false_negatives_1: 1015.0000\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 40s 243ms/step - loss: 0.9982 - accuracy: 0.7573 - true_positives_1: 898.0000 - true_negatives_1: 7496.0000 - false_positives_1: 133.0000 - false_negatives_1: 559.0000\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 40s 247ms/step - loss: 0.7842 - accuracy: 0.8205 - true_positives_1: 1043.0000 - true_negatives_1: 7516.0000 - false_positives_1: 113.0000 - false_negatives_1: 414.0000\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 40s 248ms/step - loss: 0.6021 - accuracy: 0.8867 - true_positives_1: 1146.0000 - true_negatives_1: 7567.0000 - false_positives_1: 62.0000 - false_negatives_1: 311.0000\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 40s 248ms/step - loss: 0.5424 - accuracy: 0.8852 - true_positives_1: 1172.0000 - true_negatives_1: 7572.0000 - false_positives_1: 57.0000 - false_negatives_1: 285.0000\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 40s 248ms/step - loss: 0.5155 - accuracy: 0.9022 - true_positives_1: 1199.0000 - true_negatives_1: 7577.0000 - false_positives_1: 52.0000 - false_negatives_1: 258.0000\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 40s 247ms/step - loss: 0.4376 - accuracy: 0.9361 - true_positives_1: 1259.0000 - true_negatives_1: 7614.0000 - false_positives_1: 15.0000 - false_negatives_1: 198.0000\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 40s 248ms/step - loss: 0.4208 - accuracy: 0.9391 - true_positives_1: 1270.0000 - true_negatives_1: 7620.0000 - false_positives_1: 9.0000 - false_negatives_1: 187.0000\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 40s 247ms/step - loss: 0.4378 - accuracy: 0.9307 - true_positives_1: 1265.0000 - true_negatives_1: 7618.0000 - false_positives_1: 11.0000 - false_negatives_1: 192.0000\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 40s 247ms/step - loss: 0.3904 - accuracy: 0.9384 - true_positives_1: 1273.0000 - true_negatives_1: 7627.0000 - false_positives_1: 2.0000 - false_negatives_1: 184.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f25be549a80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set RoBERTa base as trainable and retrain (lower learning rate and more epochs)\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-5, epsilon=1e-8)\n",
    "loss = tf_keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "metric = [tf_keras.metrics.CategoricalAccuracy('accuracy'), tf_keras.metrics.TruePositives(), tf_keras.metrics.TrueNegatives(), tf_keras.metrics.FalsePositives(), tf_keras.metrics.FalseNegatives()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics (precision, recall, f1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 7s 134ms/step\n"
     ]
    }
   ],
   "source": [
    "# get results\n",
    "pharo_test_results = model.predict(pharo_test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60526316 0.53488372 0.56790123]\n",
      " [0.95495495 0.8907563  0.92173913]\n",
      " [0.63461538 0.63461538 0.63461538]\n",
      " [1.         0.25       0.4       ]\n",
      " [0.92307692 0.8        0.85714286]\n",
      " [0.74418605 0.74418605 0.74418605]\n",
      " [0.2        0.1        0.13333333]]\n",
      "0.7231566381505182\n",
      "0.5649202077968933\n",
      "0.6084168552294125\n"
     ]
    }
   ],
   "source": [
    "# calculate and output metrics\n",
    "def getMetrics(y_true, y_pred):\n",
    "    TP = [0]*7\n",
    "    TN = [0]*7\n",
    "    FP = [0]*7\n",
    "    FN = [0]*7\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(7):\n",
    "        pred_col = y_pred[:, i].tolist()\n",
    "        true_col = np.array(y_true.labels.tolist())[:, i].tolist()\n",
    "\n",
    "        for j in range(len(pred_col)):\n",
    "            pred_col[j] = int(pred_col[j] > .5)\n",
    "\n",
    "        for pair in zip(true_col, pred_col):\n",
    "            if pair[0] == pair[1]:\n",
    "                if pair[0] == 1:\n",
    "                    TP[i] += 1\n",
    "                else:\n",
    "                    TN[i] += 1\n",
    "            else:\n",
    "                if pair[1] == 1:\n",
    "                    FP[i] += 1\n",
    "                else:\n",
    "                    FN[i] += 1\n",
    "        precision = TP[i]/(TP[i]+FP[i])\n",
    "        recall = TP[i]/(TP[i]+FN[i])\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "        results.append([precision, recall, f1])\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "results = np.array(getMetrics(pharo_test, pharo_test_results))\n",
    "print(results)\n",
    "print(sum(results[:, 0])/7)\n",
    "print(sum(results[:, 1])/7)\n",
    "print(sum(results[:, 2])/7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf440",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
