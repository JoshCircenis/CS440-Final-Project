{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:53:53.455484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733360033.469329 1622800 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733360033.473533 1622800 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:53:53.489043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tf_keras\n",
    "from keras import layers\n",
    "from transformers import RobertaTokenizerFast, RobertaConfig, TFRobertaModel\n",
    "\n",
    "gpu = 1\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733360038.553057 1622800 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78101 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:c1:00.0, compute capability: 8.0\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/codebert-base'\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, config=config)\n",
    "transformer_model = TFRobertaModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   1246456   ['attention_mask[0][0]',      \n",
      " r)                          ngAndCrossAttentions(last_   32         'input_ids[0][0]']           \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)     (None, 768)                  0         ['roberta[0][1]']             \n",
      "                                                                                                  \n",
      " python (Dense)              (None, 5)                    3845      ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124649477 (475.50 MB)\n",
      "Trainable params: 124649477 (475.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert = transformer_model.layers[0]\n",
    "\n",
    "# Create input layer for tokenized data\n",
    "input_ids = tf_keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "attention_mask = tf_keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32') \n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "# inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Load the bert model as a layer\n",
    "bert_model = bert(inputs)[1]\n",
    "dropout = tf_keras.layers.Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "outputs = tf_keras.layers.Dense(units=5, activation=\"softmax\", kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), name='python')(pooled_output)\n",
    "model = tf_keras.models.Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'java_train': 'data/java_train-00000-of-00001.parquet', 'java_test': 'data/java_test-00000-of-00001.parquet', 'python_train': 'data/python_train-00000-of-00001.parquet', 'python_test': 'data/python_test-00000-of-00001.parquet', 'pharo_train': 'data/pharo_train-00000-of-00001.parquet', 'pharo_test': 'data/pharo_test-00000-of-00001.parquet'}\n",
    "\n",
    "python_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_train\"])\n",
    "python_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer):\n",
    "    # Tokenize data and truncate/pad to 512\n",
    "    inputs = tokenizer(df, padding='max_length', truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Tokenize all data\n",
    "python_train_inputs = tokenize_data(python_train.combo.tolist(), tokenizer)\n",
    "python_test_inputs = tokenize_data(python_test.combo.tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels in list form\n",
    "python_train_labels = python_train.labels.tolist()\n",
    "python_test_labels = python_test.labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from the preprocessed data\n",
    "def create_tf_dataset(inputs, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((dict(inputs), labels))\n",
    "\n",
    "python_train_dataset = create_tf_dataset(python_train_inputs, python_train_labels)\n",
    "python_test_dataset = create_tf_dataset(python_test_inputs, python_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   1246456   ['attention_mask[0][0]',      \n",
      " r)                          ngAndCrossAttentions(last_   32         'input_ids[0][0]']           \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)     (None, 768)                  0         ['roberta[0][1]']             \n",
      "                                                                                                  \n",
      " python (Dense)              (None, 5)                    3845      ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124649477 (475.50 MB)\n",
      "Trainable params: 3845 (15.02 KB)\n",
      "Non-trainable params: 124645632 (475.49 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with RoBERTa base frozen\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-5, epsilon=1e-8)\n",
    "loss = tf_keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "metric = [tf_keras.metrics.CategoricalAccuracy('accuracy'), tf_keras.metrics.TruePositives(), tf_keras.metrics.TrueNegatives(), tf_keras.metrics.FalsePositives(), tf_keras.metrics.FalseNegatives()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/top/students/UNGRAD/Other/jecircen/home/.conda/envs/tf440/lib/python3.10/site-packages/tf_keras/src/backend.py:5577: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733360066.488272 1684155 service.cc:148] XLA service 0x7fb098c95a50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733360066.488295 1684155 service.cc:156]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-12-04 17:54:26.493346: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1733360066.509969 1684155 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1733360066.578667 1684155 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 33s 105ms/step - loss: 1.7025 - accuracy: 0.3047 - true_positives: 0.0000e+00 - true_negatives: 7370.0000 - false_positives: 0.0000e+00 - false_negatives: 2050.0000\n",
      "Epoch 2/5\n",
      "236/236 [==============================] - 25s 105ms/step - loss: 1.6869 - accuracy: 0.2999 - true_positives: 0.0000e+00 - true_negatives: 7370.0000 - false_positives: 0.0000e+00 - false_negatives: 2050.0000\n",
      "Epoch 3/5\n",
      "236/236 [==============================] - 25s 105ms/step - loss: 1.6842 - accuracy: 0.3259 - true_positives: 0.0000e+00 - true_negatives: 7370.0000 - false_positives: 0.0000e+00 - false_negatives: 2050.0000\n",
      "Epoch 4/5\n",
      "236/236 [==============================] - 16s 69ms/step - loss: 1.6841 - accuracy: 0.3174 - true_positives: 0.0000e+00 - true_negatives: 7370.0000 - false_positives: 0.0000e+00 - false_negatives: 2050.0000\n",
      "Epoch 5/5\n",
      "236/236 [==============================] - 11s 48ms/step - loss: 1.6779 - accuracy: 0.3163 - true_positives: 0.0000e+00 - true_negatives: 7370.0000 - false_positives: 0.0000e+00 - false_negatives: 2050.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7fb1cdba3f70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle and batch the training data use autotune for faster execution\n",
    "train_dataset = python_train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "236/236 [==============================] - 86s 252ms/step - loss: 1.3456 - accuracy: 0.5308 - true_positives_1: 637.0000 - true_negatives_1: 7219.0000 - false_positives_1: 151.0000 - false_negatives_1: 1413.0000\n",
      "Epoch 2/10\n",
      "236/236 [==============================] - 58s 247ms/step - loss: 0.9475 - accuracy: 0.7091 - true_positives_1: 1170.0000 - true_negatives_1: 7165.0000 - false_positives_1: 205.0000 - false_negatives_1: 880.0000\n",
      "Epoch 3/10\n",
      "236/236 [==============================] - 58s 246ms/step - loss: 0.7026 - accuracy: 0.8041 - true_positives_1: 1427.0000 - true_negatives_1: 7221.0000 - false_positives_1: 149.0000 - false_negatives_1: 623.0000\n",
      "Epoch 4/10\n",
      "236/236 [==============================] - 58s 247ms/step - loss: 0.5044 - accuracy: 0.8806 - true_positives_1: 1651.0000 - true_negatives_1: 7281.0000 - false_positives_1: 89.0000 - false_negatives_1: 399.0000\n",
      "Epoch 5/10\n",
      "236/236 [==============================] - 58s 248ms/step - loss: 0.3759 - accuracy: 0.9209 - true_positives_1: 1776.0000 - true_negatives_1: 7311.0000 - false_positives_1: 59.0000 - false_negatives_1: 274.0000\n",
      "Epoch 6/10\n",
      "236/236 [==============================] - 58s 247ms/step - loss: 0.3631 - accuracy: 0.9294 - true_positives_1: 1804.0000 - true_negatives_1: 7321.0000 - false_positives_1: 49.0000 - false_negatives_1: 246.0000\n",
      "Epoch 7/10\n",
      "236/236 [==============================] - 38s 162ms/step - loss: 0.3433 - accuracy: 0.9305 - true_positives_1: 1805.0000 - true_negatives_1: 7317.0000 - false_positives_1: 53.0000 - false_negatives_1: 245.0000\n",
      "Epoch 8/10\n",
      "236/236 [==============================] - 27s 113ms/step - loss: 0.2835 - accuracy: 0.9496 - true_positives_1: 1844.0000 - true_negatives_1: 7346.0000 - false_positives_1: 24.0000 - false_negatives_1: 206.0000\n",
      "Epoch 9/10\n",
      "236/236 [==============================] - 27s 113ms/step - loss: 0.2715 - accuracy: 0.9496 - true_positives_1: 1849.0000 - true_negatives_1: 7350.0000 - false_positives_1: 20.0000 - false_negatives_1: 201.0000\n",
      "Epoch 10/10\n",
      "236/236 [==============================] - 27s 113ms/step - loss: 0.2469 - accuracy: 0.9517 - true_positives_1: 1863.0000 - true_negatives_1: 7358.0000 - false_positives_1: 12.0000 - false_negatives_1: 187.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7fb1ce211db0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set RoBERTa base as trainable and retrain (lower learning rate and more epochs)\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-5, epsilon=1e-8)\n",
    "loss = tf_keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "metric = [tf_keras.metrics.CategoricalAccuracy('accuracy'), tf_keras.metrics.TruePositives(), tf_keras.metrics.TrueNegatives(), tf_keras.metrics.FalsePositives(), tf_keras.metrics.FalseNegatives()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics (precision, recall, f1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 4s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "# get results\n",
    "python_test_results = model.predict(python_test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84210526 0.66115702 0.74074074]\n",
      " [0.81395349 0.8203125  0.81712062]\n",
      " [0.39285714 0.26829268 0.31884058]\n",
      " [0.6        0.46875    0.52631579]\n",
      " [0.71428571 0.79268293 0.75144509]]\n",
      "0.672640321734569\n",
      "0.6022390269098972\n",
      "0.6308925638395732\n"
     ]
    }
   ],
   "source": [
    "# calculate and output metrics\n",
    "def getMetrics(y_true, y_pred):\n",
    "    TP = [0]*5\n",
    "    TN = [0]*5\n",
    "    FP = [0]*5\n",
    "    FN = [0]*5\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(5):\n",
    "        pred_col = y_pred[:, i].tolist()\n",
    "        true_col = np.array(y_true.labels.tolist())[:, i].tolist()\n",
    "\n",
    "        for j in range(len(pred_col)):\n",
    "            pred_col[j] = int(pred_col[j] > .5)\n",
    "\n",
    "        for pair in zip(true_col, pred_col):\n",
    "            if pair[0] == pair[1]:\n",
    "                if pair[0] == 1:\n",
    "                    TP[i] += 1\n",
    "                else:\n",
    "                    TN[i] += 1\n",
    "            else:\n",
    "                if pair[1] == 1:\n",
    "                    FP[i] += 1\n",
    "                else:\n",
    "                    FN[i] += 1\n",
    "        precision = TP[i]/(TP[i]+FP[i])\n",
    "        recall = TP[i]/(TP[i]+FN[i])\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "        results.append([precision, recall, f1])\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "results = np.array(getMetrics(python_test, python_test_results))\n",
    "print(results)\n",
    "print(sum(results[:, 0])/5)\n",
    "print(sum(results[:, 1])/5)\n",
    "print(sum(results[:, 2])/5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf440",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
