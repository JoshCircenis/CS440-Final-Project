{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tf_keras\n",
    "from keras import layers\n",
    "from transformers import RobertaTokenizerFast, RobertaConfig, TFRobertaModel\n",
    "\n",
    "gpu = 1\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/codebert-base'\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, config=config)\n",
    "transformer_model = TFRobertaModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   1246456   ['attention_mask[0][0]',      \n",
      " r)                          ngAndCrossAttentions(last_   32         'input_ids[0][0]']           \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)     (None, 768)                  0         ['roberta[0][1]']             \n",
      "                                                                                                  \n",
      " java (Dense)                (None, 7)                    5383      ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124651015 (475.51 MB)\n",
      "Trainable params: 124651015 (475.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert = transformer_model.layers[0]\n",
    "\n",
    "# Create input layer for tokenized data\n",
    "input_ids = tf_keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "attention_mask = tf_keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32') \n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "# inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Load the bert model as a layer\n",
    "bert_model = bert(inputs)[1]\n",
    "dropout = tf_keras.layers.Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "outputs = tf_keras.layers.Dense(units=7, activation=\"softmax\", kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=config.initializer_range), name='java')(pooled_output)\n",
    "\n",
    "model = tf_keras.models.Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'java_train': 'data/java_train-00000-of-00001.parquet', 'java_test': 'data/java_test-00000-of-00001.parquet', 'python_train': 'data/python_train-00000-of-00001.parquet', 'python_test': 'data/python_test-00000-of-00001.parquet', 'pharo_train': 'data/pharo_train-00000-of-00001.parquet', 'pharo_test': 'data/pharo_test-00000-of-00001.parquet'}\n",
    "\n",
    "java_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_train\"])\n",
    "java_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer):\n",
    "    # Tokenize data and truncate/pad to 512\n",
    "    inputs = tokenizer(df, padding='max_length', truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Tokenize all data\n",
    "java_train_inputs = tokenize_data(java_train.combo.tolist(), tokenizer)\n",
    "java_test_inputs = tokenize_data(java_test.combo.tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels in list form\n",
    "java_train_labels = java_train.labels.tolist()\n",
    "java_test_labels = java_test.labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from the preprocessed data\n",
    "def create_tf_dataset(inputs, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((dict(inputs), labels))\n",
    "\n",
    "java_train_dataset = create_tf_dataset(java_train_inputs, java_train_labels)\n",
    "java_test_dataset = create_tf_dataset(java_test_inputs, java_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   1246456   ['attention_mask[0][0]',      \n",
      " r)                          ngAndCrossAttentions(last_   32         'input_ids[0][0]']           \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)     (None, 768)                  0         ['roberta[0][1]']             \n",
      "                                                                                                  \n",
      " java (Dense)                (None, 7)                    5383      ['pooled_output[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124651015 (475.51 MB)\n",
      "Trainable params: 5383 (21.03 KB)\n",
      "Non-trainable params: 124645632 (475.49 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with RoBERTa base frozen\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-4, epsilon=1e-8)\n",
    "loss = tf_keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "metric = [tf_keras.metrics.CategoricalAccuracy('accuracy'), tf_keras.metrics.TruePositives(), tf_keras.metrics.TrueNegatives(), tf_keras.metrics.FalsePositives(), tf_keras.metrics.FalseNegatives()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/top/students/UNGRAD/Other/jecircen/home/.conda/envs/tf440/lib/python3.10/site-packages/tf_keras/src/backend.py:5577: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952/952 [==============================] - 53s 48ms/step - loss: 1.4608 - accuracy: 0.4749 - true_positives_2: 1898.0000 - true_negatives_2: 43922.0000 - false_positives_2: 1565.0000 - false_negatives_2: 5913.0000\n",
      "Epoch 2/5\n",
      "952/952 [==============================] - 46s 48ms/step - loss: 1.4387 - accuracy: 0.4799 - true_positives_2: 2029.0000 - true_negatives_2: 43975.0000 - false_positives_2: 1512.0000 - false_negatives_2: 5782.0000\n",
      "Epoch 3/5\n",
      "952/952 [==============================] - 45s 48ms/step - loss: 1.4249 - accuracy: 0.4800 - true_positives_2: 2058.0000 - true_negatives_2: 44097.0000 - false_positives_2: 1390.0000 - false_negatives_2: 5753.0000\n",
      "Epoch 4/5\n",
      "952/952 [==============================] - 46s 48ms/step - loss: 1.4098 - accuracy: 0.4816 - true_positives_2: 2076.0000 - true_negatives_2: 44201.0000 - false_positives_2: 1286.0000 - false_negatives_2: 5735.0000\n",
      "Epoch 5/5\n",
      "952/952 [==============================] - 46s 48ms/step - loss: 1.4009 - accuracy: 0.4887 - true_positives_2: 2120.0000 - true_negatives_2: 44285.0000 - false_positives_2: 1202.0000 - false_negatives_2: 5691.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f6c7d92c4c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle and batch the training data use autotune for faster execution\n",
    "train_dataset = java_train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 11s 40ms/step - loss: 1.3121 - accuracy: 0.5171 - true_positives_2: 369.0000 - true_negatives_2: 10191.0000 - false_positives_2: 147.0000 - false_negatives_2: 1368.0000\n",
      "Java Test precision: 0.7151162790697675\n",
      "Java Test recall: 0.21243523316062177\n",
      "Java Test f1: 0.32756324900133155\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "java_test_results = model.evaluate(java_test_dataset.batch(batch_size))\n",
    "\n",
    "TP = java_test_results[-4]\n",
    "TF = java_test_results[-3]\n",
    "FP = java_test_results[-2]\n",
    "FN = java_test_results[-1]\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "f1 = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "print(f\"Java Test precision: {precision}\")\n",
    "print(f\"Java Test recall: {recall}\")\n",
    "print(f\"Java Test f1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "952/952 [==============================] - 145s 125ms/step - loss: 0.4976 - accuracy: 0.8507 - true_positives_3: 6143.0000 - true_negatives_3: 44964.0000 - false_positives_3: 523.0000 - false_negatives_3: 1668.0000\n",
      "Epoch 2/10\n",
      "952/952 [==============================] - 108s 114ms/step - loss: 0.3553 - accuracy: 0.8956 - true_positives_3: 6720.0000 - true_negatives_3: 44994.0000 - false_positives_3: 493.0000 - false_negatives_3: 1091.0000\n",
      "Epoch 3/10\n",
      "952/952 [==============================] - 108s 113ms/step - loss: 0.2734 - accuracy: 0.9240 - true_positives_3: 7019.0000 - true_negatives_3: 45099.0000 - false_positives_3: 388.0000 - false_negatives_3: 792.0000\n",
      "Epoch 4/10\n",
      "952/952 [==============================] - 108s 113ms/step - loss: 0.2324 - accuracy: 0.9375 - true_positives_3: 7155.0000 - true_negatives_3: 45172.0000 - false_positives_3: 315.0000 - false_negatives_3: 656.0000\n",
      "Epoch 5/10\n",
      "952/952 [==============================] - 108s 113ms/step - loss: 0.1890 - accuracy: 0.9518 - true_positives_3: 7293.0000 - true_negatives_3: 45238.0000 - false_positives_3: 249.0000 - false_negatives_3: 518.0000\n",
      "Epoch 6/10\n",
      "952/952 [==============================] - 107s 113ms/step - loss: 0.1664 - accuracy: 0.9595 - true_positives_3: 7371.0000 - true_negatives_3: 45304.0000 - false_positives_3: 183.0000 - false_negatives_3: 440.0000\n",
      "Epoch 7/10\n",
      "952/952 [==============================] - 108s 113ms/step - loss: 0.1395 - accuracy: 0.9674 - true_positives_3: 7432.0000 - true_negatives_3: 45348.0000 - false_positives_3: 139.0000 - false_negatives_3: 379.0000\n",
      "Epoch 8/10\n",
      "952/952 [==============================] - 107s 113ms/step - loss: 0.1320 - accuracy: 0.9715 - true_positives_3: 7461.0000 - true_negatives_3: 45368.0000 - false_positives_3: 119.0000 - false_negatives_3: 350.0000\n",
      "Epoch 9/10\n",
      "952/952 [==============================] - 108s 113ms/step - loss: 0.1242 - accuracy: 0.9761 - true_positives_3: 7500.0000 - true_negatives_3: 45393.0000 - false_positives_3: 94.0000 - false_negatives_3: 311.0000\n",
      "Epoch 10/10\n",
      "952/952 [==============================] - 107s 113ms/step - loss: 0.1584 - accuracy: 0.9664 - true_positives_3: 7428.0000 - true_negatives_3: 45337.0000 - false_positives_3: 150.0000 - false_negatives_3: 383.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f6b4cf49f90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set RoBERTa base as trainable and retrain (lower learning rate and more epochs)\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = tf_keras.optimizers.AdamW(learning_rate=5e-5, epsilon=1e-8)\n",
    "loss = tf_keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "metric = [tf_keras.metrics.CategoricalAccuracy('accuracy'), tf_keras.metrics.TruePositives(), tf_keras.metrics.TrueNegatives(), tf_keras.metrics.FalsePositives(), tf_keras.metrics.FalseNegatives()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics (precision, recall, f1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 10s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "# get results\n",
    "java_test_results = model.predict(java_test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87241003 0.89686099 0.88446656]\n",
      " [1.         1.         1.        ]\n",
      " [0.31707317 0.38235294 0.34666667]\n",
      " [0.95264624 0.79350348 0.86582278]\n",
      " [0.77678571 0.94565217 0.85294118]\n",
      " [0.90909091 0.66666667 0.76923077]\n",
      " [0.44444444 0.23529412 0.30769231]]\n",
      "0.7532072158317813\n",
      "0.7029043380326782\n",
      "0.7181171801398294\n"
     ]
    }
   ],
   "source": [
    "# calculate and output metrics\n",
    "def getMetrics(y_true, y_pred):\n",
    "    TP = [0]*7\n",
    "    TN = [0]*7\n",
    "    FP = [0]*7\n",
    "    FN = [0]*7\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(7):\n",
    "        pred_col = y_pred[:, i].tolist()\n",
    "        true_col = np.array(y_true.labels.tolist())[:, i].tolist()\n",
    "\n",
    "        for j in range(len(pred_col)):\n",
    "            pred_col[j] = int(pred_col[j] > .5)\n",
    "\n",
    "        for pair in zip(true_col, pred_col):\n",
    "            if pair[0] == pair[1]:\n",
    "                if pair[0] == 1:\n",
    "                    TP[i] += 1\n",
    "                else:\n",
    "                    TN[i] += 1\n",
    "            else:\n",
    "                if pair[1] == 1:\n",
    "                    FP[i] += 1\n",
    "                else:\n",
    "                    FN[i] += 1\n",
    "        precision = TP[i]/(TP[i]+FP[i])\n",
    "        recall = TP[i]/(TP[i]+FN[i])\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "        results.append([precision, recall, f1])\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "results = np.array(getMetrics(java_test, java_test_results))\n",
    "print(results)\n",
    "print(sum(results[:, 0])/7)\n",
    "print(sum(results[:, 1])/7)\n",
    "print(sum(results[:, 2])/7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf440",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
